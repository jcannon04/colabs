{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "In4z5JNjnAKl",
        "Z9_xewC_e9kn",
        "LSO-DXHy3wFC",
        "hc2G8TxOfMjJ",
        "VzKzfxV1f9wn",
        "5udg7MTiuyc7"
      ],
      "authorship_tag": "ABX9TyOHkJtVEL14I+beaQqBd5uk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcannon04/colabs/blob/main/Workshop_ChatGPT_Python_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Integration Workshop\n",
        "## ChatGPT API Basics w/ Python\n"
      ],
      "metadata": {
        "id": "o132ZCCpdhpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Started With Google Colab and Jupyter Notebooks\n",
        "\n"
      ],
      "metadata": {
        "id": "In4z5JNjnAKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The code and instructions for this workshop are written entirely in **Google Colab**, which is a free service by Google that lets you write and run Python code in your browser. It's built on **Jupyter Notebooks**, and there is no setup required. Colab gives you access to computing resources for free and allows you to easily share your work with others. It's designed to be straightforward to use, with guides and tutorials if you need help getting started. There are some basics that you need to know to follow along with this guide. If you are already familiar with Colab and Jupyter Notebooks, you're welcome to skip to the next cell.\n",
        "\n",
        "If not here is the basic information you'll need to get through this workshop. You can edit cells like this one by double-clicking to enter edit mode and start making changes. This is a text cell that supports markdown, HTML, and normal text. The other type of cell is a code cell, which you can use to write Python, shell commands, and other computer code. You can click Esc to enter command mode, and you can execute the code in any cell by hitting the play button in the top-left corner of the cell. Only the code in the cell that you run is executed, so you might run into problems if you run a cell that depends on code in a previous cell. If you get errors, running the code in previous cells may solve the issue. If you need additional help or would like to know more, here are some great resources to start with.\n",
        "\n",
        "For a general introduction and features, visit [Google Colab Home](https://colab.research.google.com/)\n",
        "\n",
        "Explore detailed features and tips at [Google Colab Overview](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "The [Jupyter Notebook Docs](https://jupyter-notebook.readthedocs.io/en/stable/user-documentation.html) offers extensive information on Jupyter Notebooks.\n",
        "\n",
        "For various guides and tutorials, check out [Colab Notebook Guides](https://colab.research.google.com/notebooks/)"
      ],
      "metadata": {
        "id": "QDIsddlnqq7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shell commands in Jupyter Notebooks\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z9_xewC_e9kn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`pip` is Python's package manager and command line tool. In Colab, you can start a line with `!` to issue a shell command. We will use `pip install` to download and install our dependencies for this project and the 'pip list' command to see what other packages Colab has already installed for us."
      ],
      "metadata": {
        "id": "NR4rIGFoqhUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install dependencies\n",
        "!pip install openai\n",
        "!pip list\n",
        "#!pip install numpy"
      ],
      "metadata": {
        "id": "92uTI026eBVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obtaining and Using Your API Key\n",
        "\n"
      ],
      "metadata": {
        "id": "LSO-DXHy3wFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To follow along step by step with the rest of the workshop, you'll need to signup and get an API key at [OpenAI's Website](openai.com) and provide your credit card details. The 'gpt-3.5-turbo model' used in our workshop is cost-effective, allowing you to experiment for just a few cents. However, it's important to monitor your usage and manage spending limits through the Usage section in your OpenAI account.\n",
        "You can visit OpenAI's [pricing page](https://openai.com/pricing) for more details on tokens and pricing for different models.\n",
        "\n",
        "Once you have your api key you'll want to add it to this notebook's secrets so you can you use it in your code and not have it exposed on the internet for people to use and charge your account.\n",
        "\n",
        "1. Click the key Icon on the left side of your screen\n",
        "2. Click add secret, and type OPENAI_API_KEY in the name field then paste your api key from OpenAI in the value field.\n",
        "3. Finally click to turn on the switch for Notebook Access\n",
        "\n",
        "Now we can use the key in our code and save it to variable with the following:\n",
        "```\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "```"
      ],
      "metadata": {
        "id": "t7RIlLPyq03C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic ChatGPT API usage"
      ],
      "metadata": {
        "id": "hc2G8TxOfMjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll learn how to use the ChatGPT API via the openai python module. We start by importing necessary libraries and securely grabbing our API key.\n",
        "\n",
        "```\n",
        "# import openai library for \"easy\" api calls\n",
        "from openai import OpenAI\n",
        "\n",
        "# import user data from colab and save our api key to a variable\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "```\n",
        "\n",
        "Then, we create a \"client\" to communicate with the API by instantiating an object of the `OpenAI` class that we imported from the `openai` module.\n",
        "\n",
        "\n",
        "```\n",
        "# create client to talk to api\n",
        "client = OpenAI(\n",
        "    api_key = api_key\n",
        ")\n",
        "```\n",
        "\n",
        "Now we make a call to the ChatGPT api using the client that we created\n",
        "```\n",
        "# 3 types of messages system, user, assistant\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a personal trainer and diet and exercise expert\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you help me reach my fitness goals?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Absolutely, I can help guide you toward reaching your fitness goals! To get started, tell me a little about yourself, and your fitness goals\"},\n",
        "    {\"role\": \"user\", \"content\": \"I am a 6'0\\\" 200lb, 35 year old male. My fitness goals are getting to an appropriate weight and having more energy for my daily life\"},\n",
        "\n",
        ")\n",
        "```\n",
        "In the above code, we use the `client.chat.completions.create()` method to make an API call to the chat completions endpoint via the openai module. We passed this method two arguments: the model that we want to use, in this case, \"gpt-3.5-turbo\", and a list of messages. A single message in Python land is a dictionary with role and content keys, where the value of the role can be \"system\", \"user\", or \"assistant\", and the value of the content is a string representing the actual message content.\n",
        "\n",
        "`{\"role\": \"system\", \"content\": \"You are a personal trainer and diet and exercise expert\"}`\n",
        "\n",
        "\n",
        "The message role represents the role of the message author. Here is a breakdown of the types of roles with their general descriptions:\n",
        "\n",
        "* **System**: A system message is designed to guide or modify the behavior of the assistant.In our case, we instruct the assistant to act as a personal trainer. This type of message sets up the context or rules under which the assistant operates.\n",
        "\n",
        "* **Assistant**: This role is used to represent messages generated by ChatGPT. These types of messages are usually responses to user prompts. However, It's possible to predefine assistant messages, helping to shape the assistant's responses. This technique mirrors the concept of few-shot prompt engineering, where examples are provided to guide the assistant's output towards a desired format.\n",
        "\n",
        "* **User**: Messages with the user role represent prompts provided by the person interacting with the assistant. These inputs drive the conversation, prompting the assistant to generate responses based on the user's queries, commands, or statements.\n",
        "\n",
        "Side note, their is actualy a fourth role called **function** that has a more specialized use case. Don't worry, we will be taking a look at functions later in this workshop, but for now these are the only roles you need to know for basic API usgage.\n",
        "\n",
        "\n",
        "Finally, run the cell below and let's take a look at what we get back as a response.\n",
        "\n",
        "`print(response)`\n"
      ],
      "metadata": {
        "id": "zv0a1hFMqWgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import openai library for \"easy\" api calls\n",
        "from openai import OpenAI\n",
        "\n",
        "# import user data from colab and grab the api key\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# create \"client\" to talk to api\n",
        "client = OpenAI(\n",
        "    api_key = api_key\n",
        ")\n",
        "\n",
        "# 3 types of messages system, user, assistant\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a personal trainer and diet and exercise expert\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you help me reach my fitness goals?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Absolutely, I can help guide you toward reaching your fitness goals! To get started, tell me a little about yourself, and your fitness goals\"},\n",
        "    {\"role\": \"user\", \"content\": \"I am a 6'0\\\" 200lb, 35 year old male. My fitness goals are getting to an appropriate weight and having more energy for my daily life\"},\n",
        "  ]\n",
        ")\n",
        "\n",
        "# lets see what we get back!\n",
        "print(response)"
      ],
      "metadata": {
        "id": "4T1XaiaQfcx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatGPT API Response Format\n",
        "\n"
      ],
      "metadata": {
        "id": "VzKzfxV1f9wn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `client.chat.completions.create()` method doesn't return a simple message but instead it returns a deserialized Python `ChatCompletion` object containing the information necessary for processing within our application. This Python `ChatCompletion` object is a Python object representation of the json that would be returned directly from the api, and it has the same basic structure:\n",
        "\n",
        "```\n",
        "{\n",
        "  \"choices\": [\n",
        "    {\n",
        "      \"finish_reason\": \"stop\",\n",
        "      \"index\": 0,\n",
        "      \"message\": {\n",
        "        \"content\": \"The 2020 World Series was played in Texas at Globe Life Field in Arlington.\",\n",
        "        \"role\": \"assistant\"\n",
        "      },\n",
        "      \"logprobs\": null\n",
        "    }\n",
        "  ],\n",
        "  \"created\": 1677664795,\n",
        "  \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n",
        "  \"model\": \"gpt-3.5-turbo-0613\",\n",
        "  \"object\": \"chat.completion\",\n",
        "  \"usage\": {\n",
        "    \"completion_tokens\": 17,\n",
        "    \"prompt_tokens\": 57,\n",
        "    \"total_tokens\": 74\n",
        "  }\n",
        "}\n",
        "```\n",
        "Here's a breakdown of the components within the `ChatCompletion` object:\n",
        "\n",
        "* **choices**: An array of choice objects, where each contains:\n",
        "\n",
        "  * **finish_reason**: Indicates why the generation of text was concluded, such as reaching a limit, content being flagged innpropriate, or the model decided to use a tool.\n",
        "    * **index**: The position of this particular choice in the array, useful when multiple completions are returned.\n",
        "    * **message**: An object that holds the generated message:\n",
        "      * **content**: The actual text generated by the model.\n",
        "      * **role**: The role of the message, in this case, \"assistant\", indicating the source of the message.\n",
        "    * **logprobs**: includes the log probabilities for each token generated; null here indicates that this data was not requested.\n",
        "* **created**: A UNIX timestamp marking when the completion was created, useful for logging purposes.\n",
        "\n",
        "* **id**: A unique identifier for the completion.\n",
        "\n",
        "* **model**: Specifies the model used to generate this response.\n",
        "\n",
        "* **object**: The type of the returned object, confirming that the response is a chat completion.\n",
        "\n",
        "* **usage**: Provides metrics on token usage for the completion, useful for calculating costs.\n",
        "\n",
        "  * **completion_tokens**: The number of tokens generated in the completion.\n",
        "  * **prompt_tokens**: The number of tokens that were in the prompt.\n",
        "  * **total_tokens**: The sum of prompt and completion tokens,\n",
        "\n",
        "Since the response from the `client.chat.completions.create()` method is deserialized into a Python `ChatCompletion` object, you can access its attributes using Python's dot notation or dictionary-style indexing. We saved the `ChatCompletion` object in our `response` variable, so we can access our message content like so:\n",
        "\n",
        "`message_content = response.choices[0].message.content`"
      ],
      "metadata": {
        "id": "qr6wWqBEp4Ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# an example of a json object returned from the OpenAI API found on OpenAI's website\n",
        "# {\n",
        "#   \"choices\": [\n",
        "#     {\n",
        "#       \"finish_reason\": \"stop\",\n",
        "#       \"index\": 0,\n",
        "#       \"message\": {\n",
        "#         \"content\": \"The 2020 World Series was played in Texas at Globe Life Field in Arlington.\",\n",
        "#         \"role\": \"assistant\"\n",
        "#       },\n",
        "#       \"logprobs\": null\n",
        "#     }\n",
        "#   ],\n",
        "#   \"created\": 1677664795,\n",
        "#   \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n",
        "#   \"model\": \"gpt-3.5-turbo-0613\",\n",
        "#   \"object\": \"chat.completion\",\n",
        "#   \"usage\": {\n",
        "#     \"completion_tokens\": 17,\n",
        "#     \"prompt_tokens\": 57,\n",
        "#     \"total_tokens\": 74\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# Do I look like a robot? English please!\n",
        "message_content = response.choices[0].message.content\n",
        "print(message_content)"
      ],
      "metadata": {
        "id": "3YAQvp2vitWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accepting User Input and Continuing the Chat\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "au8jW_smk3wq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we've looked at how we can call the API once with a predetermined list of messages. That fine and all, but I want an actual chat bot that I can interact with.\n",
        "\n",
        "Moving from one-off API calls to a chatbot that can keep a conversation going involves a few key steps. We start by defining our system message to give our chatbot a role, and creating our list of messages so we can add messages as the conversation grows. We are are also importing the `json` library so we can parse our list of tools later on.\n",
        "\n",
        "```\n",
        "import json\n",
        "model = \"gpt-3.5-turbo\"\n",
        "system_message = {\"role\": \"system\", \"content\": \"You are a personal trainer and diet and exercise expert\"}\n",
        "messages = [system_message]\n",
        "```\n",
        "\n",
        "Later in this workshop, in the \"Intro to Agents\" section, we're going to give our chatbot the ability to take actions based on our conversation. We'll define a function to help with this. It's a way for the application to call functions based on a name and a dictionary of arguments that our chatbot will provide. If this sounds a bit complicated right now, that's okay. It'll make more sense when we get to that section.\n",
        "\n",
        "```\n",
        "def execute_function(function_name, arguments):\n",
        "    if function_name == \"save_file\":\n",
        "        return save_file(arguments['file_name'], arguments['file_contents'])\n",
        "```\n",
        "\n",
        "As the chat continues, we keep asking the user for their thoughts, adding both their responses and the chatbot's replies to a growing list of messages. This list helps the chatbot remember what's been said, making its answers more relevant. By repeating this process—getting input, adding it to the chat history, and asking the API for a response—we create a chat that feels more like talking to a person, where the chatbot understands and builds on what was said before\n",
        "\n",
        "For good practice let's create a function so that we can start a conversation with our bot at any point in our application without repeating ourselves.\n",
        "\n",
        "```\n",
        "def start_chat(tools=None):\n",
        "    while True:\n",
        "        prompt = input(\">> \")\n",
        "        new_message = {\"role\": \"user\", \"content\": prompt}\n",
        "        messages.append(new_message)\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            tools=tools\n",
        "        )\n",
        "\n",
        "        assistant_message = response.choices[0].message\n",
        "\n",
        "        if assistant_message.tool_calls:\n",
        "          tool_calls = assistant_message.tool_calls\n",
        "          for tool_call in tool_calls:\n",
        "            tool = tool_call.function\n",
        "            function_name = tool.name\n",
        "            arguments = json.loads(tool.arguments)\n",
        "            result = execute_function(function_name, arguments)\n",
        "            messages.append({\"role\": \"function\", \"tool_call_id\": tool_call.id, \"name\": tool_call.function.name, \"content\": result})\n",
        "        else:\n",
        "            print(assistant_message.content)\n",
        "            messages.append(assistant_message)\n",
        "\n",
        "```\n",
        "\n",
        "Okay let's break this function down.\n",
        "\n",
        "\n",
        "We create an infinite loop to keep the chat going. Every time through the loop we prompt the user for input, create a message from their prompt, and then we save that message to our list of messages.\n",
        "```\n",
        "while True:\n",
        "    prompt = input(\">> \")\n",
        "    new_message = {\"role\": \"user\", \"content\": prompt}\n",
        "    messages.append(new_message)\n",
        "```\n",
        "\n",
        "Now we make our API call using our client. We pass along the list of messages we just added the prompt to, the model that we defined at the top of the cell, and the list of tools we provided in the arguments to the `start_chat` function. We haven't defined any tools to pass yet but we can call `start_chat` with no aruguments and it will default to `None` since we defined default parameters `def start_chat(tools=None)`\n",
        "```\n",
        "response = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=messages,\n",
        "    tools=tools\n",
        ")\n",
        "```\n",
        "When we get our response back from the API we can grab the message generate at `response.choices[0].message` by our assistant and save it in a variable `assistant_message`\n",
        "\n",
        "Since later we want to provide tools to our chatbot, we need to check the response to see if it wants to use one of the tools provided `if assistant_message.tool_calls:`. If it does we will need to use the tool and then add a special type of message with the role \"function\" to our list of messages. This lets our chatbot know which tool we used, and what the result was.\n",
        "`{\"role\": \"function\", \"tool_call_id\": tool_call.id, \"name\": tool_call.function.name, \"content\": result}`\n",
        "\n",
        "If our bot doesn't provide a tool we continue the conversation as normal by adding our assistants message to the message list. Then we go back to the top of the while loop and prompt the user again.\n",
        "```\n",
        "assistant_message = response.choices[0].message\n",
        "\n",
        "if assistant_message.tool_calls:\n",
        "  tool_calls = assistant_message.tool_calls\n",
        "  for tool_call in tool_calls:\n",
        "    tool = tool_call.function\n",
        "    function_name = tool.name\n",
        "    arguments = json.loads(tool.arguments)\n",
        "    result = execute_function(function_name, arguments)\n",
        "    messages.append({\"role\": \"function\", \"tool_call_id\": tool_call.id, \"name\": tool_call.function.name, \"content\": result})\n",
        "else:\n",
        "    print(assistant_message.content)\n",
        "    messages.append(assistant_message)\n",
        "```\n",
        "\n",
        "You can now give it a try by making sure you have ran all the above code cells, uncommenting out `# start_chat()` and clicking the run button. Notice that we have a chat bot that you can keep a conversation with and that it \"remembers\" what has been said throughout the chat."
      ],
      "metadata": {
        "id": "JCbC5l5hrPZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "model = \"gpt-3.5-turbo\"\n",
        "system_message = {\"role\": \"system\", \"content\": \"You are a personal trainer and diet and exercise expert\"}\n",
        "messages = [system_message]\n",
        "\n",
        "def execute_function(function_name, arguments):\n",
        "    if function_name == \"save_file\":\n",
        "        return save_file(arguments['file_name'], arguments['file_contents'])\n",
        "\n",
        "def start_chat(tools=None):\n",
        "    while True:\n",
        "        prompt = input(\">> \")\n",
        "        new_message = {\"role\": \"user\", \"content\": prompt}\n",
        "        messages.append(new_message)\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            tools=tools\n",
        "        )\n",
        "\n",
        "        assistant_message = response.choices[0].message\n",
        "\n",
        "        if assistant_message.tool_calls:\n",
        "          tool_calls = assistant_message.tool_calls\n",
        "          for tool_call in tool_calls:\n",
        "            tool = tool_call.function\n",
        "            function_name = tool.name\n",
        "            arguments = json.loads(tool.arguments)\n",
        "            result = execute_function(function_name, arguments)\n",
        "            messages.append({\"role\": \"function\", \"tool_call_id\": tool_call.id, \"name\": tool_call.function.name, \"content\": result})\n",
        "            print(result)\n",
        "        else:\n",
        "            print(assistant_message.content)\n",
        "            messages.append(assistant_message)\n",
        "\n",
        "# start_chat()"
      ],
      "metadata": {
        "id": "H_7gywnUk-cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intro to Agents: Giving our Chatbot Tools"
      ],
      "metadata": {
        "id": "5udg7MTiuyc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The capabilities of Large Language Models (LLMs) to generate text and other data are amazing in themselves. We can now write applications with features that would have been impossible just a few years ago. However, the power of LLMs doesn't end there. LLMs like ChatGPT also have the ability to tell our applications to take actions based on the needs of the user. In ChatGPT, these capabilities are called tools or functions. Giving an LLM tools transforms it from a simple chatbot into an agent. An agent can choose to perform any function we write and describe to it. It can save files, make database queries, perform API calls to third-party applications, spin up server instances in the cloud and much, much more. The possibilites are really endless and with a little creativity you can create robots that can do almost anything.\n",
        "\n",
        "In this section, we will look at giving our chatbot one tool for demonstration purposes. While having a chatbot create a document, and save it's work to a file\n",
        "system is pretty cool, it is just one example in an infinite pool of possibilites. You can also give your chatbot more than one tool, and it can choose to use multiple tools in one response. A tool can be any function that you can create and describe in json format. Let's take a look at how to give our chatbot a tool to save files.\n",
        "\n",
        "Fist we import `os` we can interact with our file system. Then we create a base directory where the file will be saved. You can choose any directory you want but the content folder is a good place for colab notebooks.\n",
        "\n",
        "```\n",
        "# well be using os to save to files\n",
        "import os\n",
        "base_dir = '/content'\n",
        "```\n",
        "Then we define our function. The `save_file` function will accept the `file_name` and the `file_contents` and write a file with the name and contents to our `base_dir`\n",
        "\n",
        "```\n",
        "def save_file(file_name, file_contents):\n",
        "    \"\"\"\n",
        "    Saves the given string to a file on disk with the specified file name\n",
        "    in the content directory of the Google Colab environment.\n",
        "\n",
        "    Parameters:\n",
        "    - file_name: str, the name of the file to save.\n",
        "    - file_contents: str, the contents to write to the file.\n",
        "    \"\"\"\n",
        "    # Define the file path in the root directory\n",
        "    file_path = os.path.join(base_dir, file_name)\n",
        "\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(file_contents)\n",
        "\n",
        "    return f\"success! file saved to {file_name}\"\n",
        "```\n",
        "\n",
        "Now we need to describe our function to our agent so that it knows exactly how it works, and it can decide based on our conversation when to call the function. With the chatGPT API we describe our function to our chatbot using a json format\n",
        "\n",
        "```\n",
        "tools = [\n",
        "  {\n",
        "      \"type\": \"function\",\n",
        "      \"function\": {\n",
        "          \"name\": \"save_file\",\n",
        "          \"description\": \"Saves the given string to a file on disk. The file is saved in the root directory of the current working environment, such as a Google Colab session.\",\n",
        "          \"parameters\": {\n",
        "              \"type\": \"object\",\n",
        "              \"properties\": {\n",
        "                  \"file_name\": {\n",
        "                      \"type\": \"string\",\n",
        "                      \"description\": \"The name of the file to save.\"\n",
        "                  },\n",
        "                  \"file_contents\": {\n",
        "                      \"type\": \"string\",\n",
        "                      \"description\": \"The contents to write to the file.\"\n",
        "                  }\n",
        "              },\n",
        "              \"required\": [\"file_name\", \"file_contents\"]\n",
        "          }\n",
        "      }\n",
        "  }\n",
        "]\n",
        "```\n",
        "\n",
        "`tools` is the list of tools that we are going to provide to the API. In this case we just have one tool in our list but we could provide many by adding more tool objects. Each tool object is a description of a function including its name and a description of its parameters. Here is a break down of the different fields in a tool object and what they mean.\n",
        "\n",
        "* **type**: This specifies the type of tool. In this case, it's a \"function\"\n",
        "\n",
        "* **function**: This object provides detailed information about the function, including how to invoke it and what parameters it requires.\n",
        "\n",
        "  * **name**: The name of the function. This is how you'll refer to the function when you want the agent to use it.\n",
        "\n",
        "  * **description**: A human-readable description of what the function does.\n",
        "\n",
        "  * **parameters**: This object describes the inputs the function accepts.\n",
        "\n",
        "    * **type**: Indicates the structure type of the parameters.\n",
        "\n",
        "    * **properties**: This object lists all the parameters the function requires.\n",
        "\n",
        "      * **file_name**: A parameter that specifies the name of the file to save. This object contains a type and description field.\n",
        "      * **file_contents**: Another parameter. It is also has a type and description field.\n",
        "\n",
        "  * **required**: An array listing all the necessary parameters for the function to execute.\n",
        "\n",
        "Now we can start chatting with our bot by calling the `start_chat` function that we described earlier, passing in our tools array like so: `start_chat(tools)`\n",
        "\n",
        "Lets go back to our `start_chat` and `execute_function` functions defined in our last section and take a closer look at how they work.\n",
        "\n",
        "Inside of our `start_chat` function after our call to the API we are checking to see if our response indicates that chatGPT wants to call our supplied function.\n",
        "\n",
        "```\n",
        "if assistant_message.tool_calls:\n",
        "  tool_calls = assistant_message.tool_calls\n",
        "  for tool_call in tool_calls:\n",
        "    tool = tool_call.function\n",
        "    function_name = tool.name\n",
        "    arguments = json.loads(tool.arguments)\n",
        "    result = execute_function(function_name, arguments)\n",
        "    messages.append({\"role\": \"function\", \"tool_call_id\": tool_call.id, \"name\": tool_call.function.name, \"content\": result})\n",
        "else:\n",
        "    print(assistant_message.content)\n",
        "    messages.append(assistant_message)\n",
        "```\n",
        "\n",
        "If there are any tool calls listed in `assistant_message.tool_calls` then we can extract the function information and call our `execute_function` function passing in the name and arguments extracted from the response.\n",
        "\n",
        "```\n",
        "def execute_function(function_name, arguments):\n",
        "    if function_name == \"save_file\":\n",
        "        return save_file(arguments['file_name'], arguments['file_contents'])\n",
        "```\n",
        "\n",
        "This function calls our `save_file` function and passes in the file name and contents.\n",
        "\n",
        "\n",
        "We then return to our code in the `save_file` function that creates a new message and appends it to our list of messages in the chat\n",
        "\n",
        "```\n",
        "messages.append({\"role\": \"function\", \"tool_call_id\": tool_call.id, \"name\": tool_call.function.name, \"content\": result})\n",
        "```\n",
        "\n",
        "This message has the special role \"function\" to let chatGPT know that we called a function and what the result was.\n",
        "We then go back to the top of while loop and continue the conversation.\n",
        "\n",
        "Now are chat bot can keep chatting with us and it \"remembers\" the functions that it told us to call and the outcome of calling them.\n",
        "\n",
        "Let's give it a try. Ensure that we have executed all the above cells, uncomment the `#start_chat(tools)` and hit the run button. We should be able to tell our agent to generate workout and diet plans and save them to disk. There we go our own personal trainer agent using the ChatGPT API."
      ],
      "metadata": {
        "id": "YVOPWcpeagvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# well be using os to save to files\n",
        "import os\n",
        "base_dir = '/content'\n",
        "\n",
        "# create our tool function\n",
        "def save_file(file_name, file_contents):\n",
        "    \"\"\"\n",
        "    Saves the given string to a file on disk with the specified file name\n",
        "    in the root directory of the Google Colab environment.\n",
        "\n",
        "    Parameters:\n",
        "    - file_name: str, the name of the file to save.\n",
        "    - file_contents: str, the contents to write to the file.\n",
        "    \"\"\"\n",
        "    # Define the file path in the root directory\n",
        "    file_path = os.path.join(base_dir, file_name)\n",
        "\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(file_contents)\n",
        "\n",
        "    return f\"success! file saved to {file_name}\"\n",
        "\n",
        "\n",
        "# describe our functions to chatGPT with tools array\n",
        "tools = [\n",
        "  {\n",
        "      \"type\": \"function\",\n",
        "      \"function\": {\n",
        "          \"name\": \"save_file\",\n",
        "          \"description\": \"Saves the given string to a file on disk. The file is saved in the root directory of the current working environment, such as a Google Colab session.\",\n",
        "          \"parameters\": {\n",
        "              \"type\": \"object\",\n",
        "              \"properties\": {\n",
        "                  \"file_name\": {\n",
        "                      \"type\": \"string\",\n",
        "                      \"description\": \"The name of the file to save.\"\n",
        "                  },\n",
        "                  \"file_contents\": {\n",
        "                      \"type\": \"string\",\n",
        "                      \"description\": \"The contents to write to the file.\"\n",
        "                  }\n",
        "              },\n",
        "              \"required\": [\"file_name\", \"file_contents\"]\n",
        "          }\n",
        "      }\n",
        "  }\n",
        "]\n",
        "start_chat(tools)"
      ],
      "metadata": {
        "id": "Ma0zZArAvLJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Closing Thoughts"
      ],
      "metadata": {
        "id": "Sz3MlEsfvANG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is not by any means a production grade application. These are just examples of OpenAI's API usage. One major issue is the fact that we keep adding messages to our list of messages every time we enter a prompt. This is not scalable and you will run out of context length fairly quickly.\n",
        "\n",
        "The main point that I want to get across with this workshop is that there is an unlimited amount of possible applications you can build with LLM integrations. Please experiment with this code and use it in other applications to build web, mobile, and other types of apps. There are many possibilities to explore and I hope this guide will get you started experimenting. Thanks for reading!"
      ],
      "metadata": {
        "id": "lguIy0cYvJW7"
      }
    }
  ]
}